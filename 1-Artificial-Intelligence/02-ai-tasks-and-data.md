# AI Tasks and Data - Learning Notes

## Overview
This video covers AI tasks and data across three main domains: "**language**", "**audio and speech**", and "**vision**".  
It explains different types of AI tasks (text-related, audio-related, image-related, and generative AI), how data in each domain is structured, and the deep learning model architectures used to process them. Additional AI tasks such as anomaly detection, recommendations, and forecasting are also discussed.

---

## Key Concepts
- **AI Domains**: Language, Audio and Speech, Vision  
- **Language Tasks**: Text-related tasks, Generative AI tasks  
- **Speech/Audio Tasks**: Audio-related tasks, Generative AI tasks  
- **Vision Tasks**: Image-related tasks, Generative AI tasks  
- **Data Representation**: Tokenization, Padding, Embeddings, Sampling rate, Bit depth, Pixels  
- **Deep Learning Architectures**: RNN, LSTM, Transformers, CNN, YOLO, GANs, Variational Autoencoders, Waveform Models, Siamese Networks  
- **Other AI Tasks**: Anomaly detection, Recommendations, Forecasting  

---

## Detailed Notes  

### 1. Language-Related AI Tasks
- **Types**:
  - **Text-related tasks**:  
    - Input: Text  
    - Output varies depending on task  
    - Examples: detecting language, extracting entities, extracting key phrases, text translation  
  - **Generative AI tasks**:  
    - Output text is generated by a model  
    - Examples: creating stories or poems, summarizing text, answering questions  
    - Example tool: **ChatGPT**, which generates responses from training on large language models  

- **Text as Data**:
  - Text is sequential, consisting of sentences and words
  - Words must be converted to numbers → **Tokenization**  
  - Sentences vary in length → made equal through **Padding**  
  - Similarity between words/sentences measured by **dot similarity** or **cosine similarity**  
  - **Embeddings** represent words/sentences in a way that similar items are placed close together  

- **Language AI Models**:
  - Designed to understand, process, and generate natural language  
  - Trained on vast amounts of text data  
  - Common deep learning architectures:
    - **Recurrent Neural Networks (RNNs)**: process data sequentially, store hidden states  
    - **Long Short-Term Memory (LSTM)**: process sequential data, retain context with gates  
    - **Transformers**: process data in parallel, use self-attention for context  

---

### 2. Speech and Audio-Related AI Tasks
- **Types**:
  - **Audio-related tasks**: speech-to-text, speaker recognition, voice conversion  
  - **Generative AI tasks**: music composition, speech synthesis  

- **Audio as Data**:
  - Speech/audio digitized as snapshots over time  
  - **Sample rate**: number of samples per second  
    - Example: 44.1 kHz (CD quality), meaning 44,100 samples per second  
  - **Bit depth**: number of bits per sample (information richness of each sample)  
  - A single sample reveals little; multiple samples must be correlated  
    - Example: listening for a fraction of a second reveals little about a song  

- **Audio and Speech AI Models**:
  - Designed to process and understand audio data, including spoken language  
  - Architectures include:  
    - **RNNs**  
    - **LSTMs**  
    - **Transformers**  
    - **Variational Autoencoders (VAEs)**  
    - **Waveform Models**  
    - **Siamese Networks**  

---

### 3. Vision-Related AI Tasks
- **Types**:
  - **Image-related tasks**:  
    - Input: Image  
    - Output depends on task  
    - Examples: image classification, object detection, facial recognition  
    - Facial recognition widely used in surveillance, biometrics, law enforcement, social media  
  - **Generative AI tasks**:  
    - Output images generated by models  
    - Examples: contextual image creation, style transfer, high-resolution images  
    - Can generate realistic new images and videos, including 3D models of objects, components, buildings, medication, people, etc.  

- **Images as Data**:
  - Images are made of **pixels** (grayscale or color)  
  - A single pixel does not reveal an image  
  - Input and output depend on the task  

- **Vision AI Models**:
  - Architectures include:  
    - **Convolutional Neural Networks (CNNs)**: detect patterns, learn hierarchical visual features  
    - **YOLO (You Only Look Once)**: detects objects in images in one pass  
    - **Generative Adversarial Networks (GANs)**: generate realistic images  

---

### 4. Other AI Tasks
- **Anomaly Detection**  
  - Requires **time series data** (single or multivariate)  
  - Applications: fraud detection, machine failure  

- **Recommendations**  
  - Based on similarity between products or users  
  - Requires data on similar products/users  

- **Forecasting**  
  - Requires **time series data**  
  - Applications: weather forecasting, stock price prediction  

---

## Summary
The video explains AI tasks and data across three domains: **language, audio/speech, and vision**.  
- **Language tasks** involve text-related or generative tasks, processed using RNNs, LSTMs, and Transformers.  
- **Speech and audio tasks** involve recognition or generation, with data represented as samples (defined by sample rate and bit depth). Models include RNNs, LSTMs, Transformers, VAEs, waveform models, and Siamese networks.  
- **Vision tasks** involve classification, detection, recognition, or image generation, using CNNs, YOLO, and GANs.  
- Additional AI applications include **anomaly detection**, **recommendations**, and **forecasting**, all relying on appropriate data structures.  

AI models across these domains share a focus on structured data representation and deep learning architectures tailored to each type of input.
